{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *       # for datatype conversion\n",
    "from pyspark.sql.functions import *   # for col() function\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "import re\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from numpy import array\n",
    "from math import sqrt\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.ml.feature import MaxAbsScaler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.ml.clustering import BisectingKMeans\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "sqlCtx = SQLContext(sc)\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Jeopardy Calculation\") \\\n",
    "    .config(\"spark.executor.memory\", '2g') \\\n",
    "    .config('spark.executor.cores', '1') \\\n",
    "    .config('spark.cores.max', '1') \\\n",
    "    .config(\"spark.driver.memory\",'1g') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "def stem(in_vec):\n",
    "    out_vec = []\n",
    "    for t in in_vec:\n",
    "        t_stem = stemmer.stem(t)\n",
    "        out_vec.append(t_stem)       \n",
    "    return out_vec\n",
    "from pyspark.sql.types import *\n",
    "stemmer_udf = udf(lambda x: stem(x), ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading in the jeopardy data set\n",
    "jeopardy = spark.read.json(\"JEOPARDY_QUESTIONS1-Copy1.json\")\n",
    "j_categoryCount = jeopardy.groupBy(\"category\").count()\n",
    "#This is the number of categories in the dataset which have greater than 100 observations\n",
    "count100 = j_categoryCount.sort(desc(\"count\")).filter(j_categoryCount[\"count\"] > 100).count()\n",
    "#This is a list of all categories which have a count greater than 100\n",
    "top_categories = list(j_categoryCount.sort(desc(\"count\")).select(\"category\").limit(count100).toPandas().category)\n",
    "#new dataset that only contains categories that have greater than 100 osbervations\n",
    "jeo_f = jeopardy.where(col(\"category\").isin(top_categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[air_date: string, answer: string, category: string, question: string, round: string, show_number: string, value: string, stripped: string, words: array<string>, filtered: array<string>, stemmed: array<string>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stripping punctuation, tokenizing, stop word removing, and stemming for the modified dataset\n",
    "jeo_fpunc = jeo_f.withColumn(\"stripped\", f.regexp_replace(f.col(\"question\"), \"[\\!@#$%^&*)(><,';:]\", \"\"))\n",
    "jeo_fpunc.cache()\n",
    "tokenizer = Tokenizer(inputCol = \"stripped\", outputCol = \"words\")\n",
    "tokenized_f = tokenizer.transform(jeo_fpunc)\n",
    "tokenized_f.cache()\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "jeo_fStopRemoved = remover.transform(tokenized_f)\n",
    "jeo_fStopRemoved.cache()\n",
    "jeo_fStemmed = jeo_fStopRemoved.withColumn(\"stemmed\", stemmer_udf(\"filtered\"))\n",
    "jeo_fStemmed.cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression on Human Defined Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "humanclust = [9, 0, 5, 7, 9, 7, 9, 2, 7, 8, 2, 2, 3, 1, 2, 8, 3, 2, 2, 5, 6, 5, 5, 8, 8, 5, 9, 4, 0, 6,\n",
    "              9, 5, 6, 5, 7, 1, 8, 0, 7, 9, 2, 8, 9, 8, 5, 6, 4, 6, 5, 9, 9, 5, 5, 8, 2, 6, 2, 7, 6, 4,\n",
    "              4, 0, 1, 9, 7, 0, 2, 1, 9, 7, 0, 4, 4, 4, 0, 0, 4, 2, 1, 0, 8, 9, 4, 8, 0, 1, 9, 9, 8, 8,\n",
    "              9, 0, 7, 0, 7, 4, 2, 9, 4, 4, 0, 7, 9, 0, 4, 4, 3, 2, 1, 9, 8, 4, 4, 9, 5, 9, 1, 0, 2, 4,\n",
    "              1, 9, 4, 4, 9, 6, 4, 4, 5, 4, 8, 9, 6, 5, 5, 0, 2, 0, 0, 4, 8, 8, 4, 7, 2]\n",
    "human_udf = udf(lambda x: int(humanclust[top_categories.index(x)]))\n",
    "jeo_g = jeo_fStemmed.withColumn(\"human\", human_udf(jeo_f.category).cast(\"integer\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training,test = jeo_g.randomSplit([0.8,0.2], seed = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashingTF = HashingTF(inputCol=\"stemmed\", outputCol=\"rawFeatures\", numFeatures = 50000)\n",
    "idf = IDF(inputCol=hashingTF.getOutputCol(), outputCol=\"features\")\n",
    "indexer = StringIndexer(inputCol=\"human\", outputCol=\"label\")\n",
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=10, elasticNetParam=0.8,\n",
    "                       family=\"multinomial\")\n",
    "\n",
    "pipelineLR = Pipeline(stages=[hashingTF,idf,indexer,lr])\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.001,0.0025,0.005,0.0075,.01]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=pipelineLR,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(),\n",
    "                          numFolds=5)\n",
    "cvModel = crossval.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy = 0.5044232732221844\n"
     ]
    }
   ],
   "source": [
    "prediction = cvModel.transform(test)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",\n",
    "                                              metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(prediction)\n",
    "print(\"Test set accuracy = \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4624088372363486,\n",
       " 0.48117086297810086,\n",
       " 0.49684745917594786,\n",
       " 0.49839143565237204,\n",
       " 0.4908468277268364]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|       9.0|  159|\n",
      "|       8.0|  222|\n",
      "|       7.0|  240|\n",
      "|       6.0|  374|\n",
      "|       5.0|  503|\n",
      "|       4.0|  546|\n",
      "|       3.0|  698|\n",
      "|       2.0|  834|\n",
      "|       1.0|  628|\n",
      "|       0.0| 1674|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction.groupBy('prediction').count().sort(desc(\"prediction\")).show(145)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
